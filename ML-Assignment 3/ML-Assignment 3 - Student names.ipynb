{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca31d406",
   "metadata": {},
   "source": [
    "# Machine Learning - Assignment 3\n",
    "\n",
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4531893a",
   "metadata": {},
   "source": [
    "The aim of the assignment is to implement an artificial neural network (mostly) from scratch. This includes implementing or fixing the following:\n",
    "\n",
    "* Add support for additional activation functions and their derivatives.\n",
    "* Add support for loss functions and their derivative.\n",
    "* Add the use of a bias in the forward propagation.\n",
    "* Add the use of a bias in the backward propagation.\n",
    "\n",
    "In addition, you will we doing the following as well:\n",
    "\n",
    "* Test the algorithm on 3 datasets.\n",
    "* Compare neural networks with and without scaling.\n",
    "* Hyper-parameter tuning.\n",
    "\n",
    "The forward and backward propagation is made to work through a single layer, and are re-used multiple times to work for multiple layers.\n",
    "\n",
    "Follow the instructions and implement what is missing to complete the assignment. Some functions have been started to help you a little bit with the implementation.\n",
    "\n",
    "**Note:** You might need to go back and forth during your implementation of the code. The structure is set up to make implementation easier, you might find yourself going back and and forth to change something to make it easier later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb200d9",
   "metadata": {},
   "source": [
    "## Assignment preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802a64bd",
   "metadata": {},
   "source": [
    "We help you out with importing the libraries.\n",
    "\n",
    "**IMPORTANT NOTE:** You may not import any more libraries than the ones already imported!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0711fb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# We set seed to better reproduce results later on.\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1242d5e",
   "metadata": {},
   "source": [
    "## Neural Network utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c0c6ac",
   "metadata": {},
   "source": [
    "### 1) Activation functions\n",
    "\n",
    "Below is some setup for choosing activation function. Implement 2 additional activation functions, \"ReLU\" and one more of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ca10df73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def activate(activations, selected_function = \"none\"):\n",
    "    \n",
    "    if selected_function == \"none\":\n",
    "        y = activations\n",
    "    elif selected_function == \"relu\" :\n",
    "        y=np.maximum(0,activations)\n",
    "    elif selected_function == \"sigmoid\" :\n",
    "        y=1/(1+np.exp((-activations)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "158ebbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid results [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
      "Relu results [0 0 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Sigmoid results\",activate(test_data,selected_function=\"sigmoid\"))\n",
    "print(\"Relu results\",activate(test_data,selected_function=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d7f8fa",
   "metadata": {},
   "source": [
    "### 2) Activation function derivatives\n",
    "\n",
    "Neural networks need both the activation function and its derivative. Finish the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "17c00420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_activate(activations, selected_function = \"none\"):\n",
    "    if selected_function == \"none\":\n",
    "        dy = np.ones_like(activations)\n",
    "    elif selected_function == \"relu\":\n",
    "        dy=np.where(activations>0,1,0)\n",
    "    elif selected_function == \"sigmoid\":\n",
    "        y=activate(activations,selected_function=\"sigmoid\")\n",
    "        dy=y*(1-y)\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ea074e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid results [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
      "Relu results [0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([-2, -1, 0, 1, 2])\n",
    "print(\"Sigmoid results\",d_activate(test_data,selected_function=\"sigmoid\"))\n",
    "print(\"Relu results\",d_activate(test_data,selected_function=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76f72d0",
   "metadata": {},
   "source": [
    "### 3) Loss functions\n",
    "\n",
    "To penalize the network when it predicts incorrect, we need to meassure how \"bad\" the prediction is. This is done with loss-functions.\n",
    "\n",
    "Similar as with the activation functions, the loss function needs its derivative as well.\n",
    "\n",
    "Finish the MSE_loss (Mean Squared Error loss), as well as adding one additional loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4ff5b095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the loss for a set of predictions y_hat compared to a set of real valyes y\n",
    "def MSE_loss(y_hat, y):\n",
    "    square_error=(y_hat-y)**2\n",
    "    loss = np.mean(square_error)\n",
    "    return loss\n",
    "\n",
    "#Binary Cross-Entropy because we use Sigmoid Function above for activation\n",
    "def Binary_CE(y_hat, y):\n",
    "    epsilon=1e-15\n",
    "    y_hat=np.clip(y_hat,epsilon,1-epsilon)\n",
    "    loss = -np.mean(y*np.log(y_hat)+(1-y)*np.log(1-y_hat))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a965be68",
   "metadata": {},
   "source": [
    "The derivatives of the loss is with respect to the predicted value **y_hat**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dea0d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_MSE_loss(y_hat, y):\n",
    "    dy = 2 * (y_hat - y) / y.size\n",
    "    return dy\n",
    "\n",
    "#Binary Cross-Entropy because we use Sigmoid Function above for activation\n",
    "def d_Binary_CE(y_hat, y):\n",
    "    epsilon=1e-15\n",
    "    y_hat=np.clip(y_hat,epsilon,1-epsilon)\n",
    "    dy = -(y/y_hat - (1-y)/(1-y_hat)) / y.size\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "25d4e7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.06666667  0.13333333 -0.13333333]\n",
      "[-0.37037037  0.41666667 -0.41666667]\n"
     ]
    }
   ],
   "source": [
    "y_hat = np.array([0.9, 0.2, 0.8])\n",
    "y     = np.array([1.0, 0.0, 1.0])\n",
    "print(d_MSE_loss(y_hat, y))\n",
    "print(d_Binary_CE(y_hat, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badfeb3b",
   "metadata": {},
   "source": [
    "### 4) Forward propagation\n",
    "\n",
    "The first \"fundamental\" function for neural networks is to be able to propagate the data forward through the neural network. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "32b521f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def propagate_forward(weights, activations, bias, activation_function=\"none\"):\n",
    "    \n",
    "    # TODO: Add support for the use of bias\n",
    "\n",
    "    dot_product = np.dot(weights, activations) + bias\n",
    "    new_activations = activate(dot_product, activation_function)\n",
    "    return new_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8ec28d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[3]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([[2, -1]])   # shape (1,2)\n",
    "activations = np.array([3, 4])  # input\n",
    "bias = np.array([1])  \n",
    "print(bias)\n",
    "out = propagate_forward(weights, activations, bias, activation_function=\"none\")\n",
    "print(out)          # bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4abe8c",
   "metadata": {},
   "source": [
    "### 5) Back-propagation\n",
    "\n",
    "To be able to train a neural network, we need to be able to propagate the loss backwards and update the weights. We will implement this function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "83b2ad5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates the backward gradients that are passed throught the layer in the backward pass.\n",
    "# Returns both the derivative of the loss in respect to the weights and the input signal (activations).\n",
    "\n",
    "def propagate_backward(weights, activations, dl_dz, bias, activation_function=\"none\"):\n",
    "    # NOTE: dl_dz is the derivative of the loss based on the previous layers activations/outputs\n",
    "\n",
    "    # TODO: Add support for the use of bias\n",
    "\n",
    "    dot_product = np.dot(activations, weights) + bias\n",
    "\n",
    "    d_loss = d_activate(dot_product, activation_function) * dl_dz\n",
    "    d_weights = np.dot(activations.T, d_loss)\n",
    "    d_bias = np.sum(d_loss, axis=0, keepdims=True)\n",
    "    d_activations = np.dot(d_loss, weights.T)\n",
    "    \n",
    "    return d_weights,d_bias, d_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d9881166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW = [[1 1]\n",
      " [2 2]]\n",
      "db = [[1 1]]\n",
      "dA = [[5 9]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "activations = np.array([[1, 2]])     # shape (1,2)\n",
    "weights = np.array([[2,3],\n",
    "                    [4,5]])\n",
    "bias        = np.array([[0, 0]])     # shape (1,2)\n",
    "dl_dz       = np.array([[1, 1]])     # shape (1,2)\n",
    "\n",
    "dW, db, dA = propagate_backward(weights, activations, dl_dz, bias, activation_function=\"none\")\n",
    "\n",
    "print(\"dW =\", dW)\n",
    "print(\"db =\", db)\n",
    "print(\"dA =\", dA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73645779",
   "metadata": {},
   "source": [
    "## Neural network implementation\n",
    "\n",
    "### 6) Fixing the neural network\n",
    "\n",
    "Below is a class implementation of a MLP neural network. This implementation is still lacking several areas that are needed for the network to be robust and function well. Your task is to improve and fix it with the following:\n",
    "\n",
    "1. Add a bias to the activation functions, and make sure the bias is also updated during training. \n",
    "2. Add a function that trains the network using minibatches (such that the neural network trains on a few samples at a time). \n",
    "3. Make use of an validation set in the training function. The model should stop training when the loss starts to increase for the validatin set. This feature should be able to be turned on and off to test the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c550aa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(object):\n",
    "    \n",
    "    # Setup all parameters and activation functions.\n",
    "    # This function runs directly when a new instance of this class is created.\n",
    "    def __init__ (self, input_dim, output_dim, neurons = []):\n",
    "\n",
    "        # NOTE: The \"neurons\" parameter is given as a list.\n",
    "        # E.g., [4, 8, 4] means 4 neurons in layer 1, 8 neurons in layer 2 etc...\n",
    "\n",
    "        # TODO: Add support for bias for each neuron in the code below.\n",
    "        \n",
    "        self.weights = [np.random.normal(0,2,(n,m)) for n,m in\n",
    "                   zip([input_dim] + neurons, neurons + [output_dim])]\n",
    "        \n",
    "        self.activation_functions = [\"relu\"] * len(neurons) + [\"none\"]\n",
    "    \n",
    "    \n",
    "    # Predict the input throught the network and calculate the output.\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO: Add support for a bias for each neuron in the code below.\n",
    "        for layer_weights, layer_activation_function in zip(self.weights, self.activation_functions):\n",
    "            x = propagate_forward(layer_weights, x, layer_activation_function)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    # Adjust the weights in the network to better fit the desired output (y), given the input (x).\n",
    "    # The weight updates are happening \"in-place\", thus we are only returning the loss from this function.\n",
    "    # Note that this function can handle a variable size of the input (x), both full datasets or smaller parts of the dataset.\n",
    "    def adjust_weights(self, x, y, learning_rate=1e-4):\n",
    "                \n",
    "        # TODO: Add support for a bias for each neuron and make sure these are learnt as well in the code below.\n",
    "\n",
    "        activation = x\n",
    "        activation_history = [] # NOTE: We need the previous (or intermediate) activations to make use of the \"chain rule\" (see lecture notes).\n",
    "        \n",
    "        for layer_weights, layer_activation_function in zip(self.weights, self.activation_functions):\n",
    "            activation_history.append(activation)\n",
    "            activation = propagate_forward(layer_weights, activation, layer_activation_function)\n",
    "           \n",
    "        # NOTE: The \"activation\" variable is changing as we go forward in the neural network.\n",
    "\n",
    "        loss = MSE_loss(activation,y)\n",
    "        d_activations = d_MSE_loss(activation,y) # NOTE: The final output can be \"seen as\" the final activations, thus the name.\n",
    "        \n",
    "        for layer_weights, layer_activation_function, previous_activations in reversed(list(zip(self.weights, self.activation_functions, activation_history))):\n",
    "\n",
    "            d_weights, d_activations = propagate_backward(layer_weights, previous_activations, d_activations, layer_activation_function)\n",
    "\n",
    "            # NOTE: Here is where the weight update is happening.\n",
    "            layer_weights -= learning_rate * d_weights\n",
    "                        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    # A function for the training of the network.\n",
    "    def train_net(self, x, y, batch_size=32, epochs=100, learning_rate=1e-4, use_validation_data=False):\n",
    "        \n",
    "        # TODO: Add a training loop where the weights and biases of the network is learnt over several epochs.\n",
    "\n",
    "        # TODO: Add support for mini batches. That is, in each epoch the data should be split into several\n",
    "        #       smaller subsets and the model should be trained on each of these subsets one at a time.\n",
    "\n",
    "        # TODO: Implement the use of validation data, that is, splitting the training data into training data and validation data.\n",
    "        #       The validation data should be used to stop the training when the model stops to generalise and starts to overfit.\n",
    "        #       This feature should be able to be turned on and off to test the difference.\n",
    "\n",
    "        # NOTE: Make use of previously implemented functions here.\n",
    "\n",
    "        ...\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e0cc71",
   "metadata": {},
   "source": [
    "## Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb678ea9",
   "metadata": {},
   "source": [
    "### 7) Simple test\n",
    "\n",
    "In this a very simple test for you to use and toy around with before using the datasets.\n",
    "\n",
    "Make sure to test both the **adjust_weights** function and the **train_net** function. What is the difference between the two?\n",
    "\n",
    "Also, be sure to **plot the loss for each epoch** to see how the network training is progressing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3d0d3157",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,18) and (1000,4) not aligned: 18 (dim 1) != 1000 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      8\u001b[39m y = np.dot(x,k) + \u001b[32m0.1\u001b[39m + np.random.normal(\u001b[32m0\u001b[39m,\u001b[32m0.01\u001b[39m,(n,\u001b[32m1\u001b[39m))\n\u001b[32m     11\u001b[39m nn = NeuralNet(d, \u001b[32m1\u001b[39m, [\u001b[32m18\u001b[39m, \u001b[32m12\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m loss_1 = [\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1000\u001b[39m)] \n\u001b[32m     15\u001b[39m loss_2 = train_net(...) \u001b[38;5;66;03m# TODO: Use the train_net function to compare with the \"adjust_weights\" function.\u001b[39;00m\n\u001b[32m     17\u001b[39m plt.plot(loss_1)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[124]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mNeuralNet.adjust_weights\u001b[39m\u001b[34m(self, x, y, learning_rate)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_weights, layer_activation_function \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m.weights, \u001b[38;5;28mself\u001b[39m.activation_functions):\n\u001b[32m     39\u001b[39m     activation_history.append(activation)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     activation = \u001b[43mpropagate_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_activation_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# NOTE: The \"activation\" variable is changing as we go forward in the neural network.\u001b[39;00m\n\u001b[32m     44\u001b[39m loss = MSE_loss(activation,y)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mpropagate_forward\u001b[39m\u001b[34m(weights, activations, bias, activation_function)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpropagate_forward\u001b[39m(weights, activations, bias, activation_function=\u001b[33m\"\u001b[39m\u001b[33mnone\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# TODO: Add support for the use of bias\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     dot_product = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m)\u001b[49m + bias\n\u001b[32m      6\u001b[39m     new_activations = activate(dot_product, activation_function)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_activations\n",
      "\u001b[31mValueError\u001b[39m: shapes (4,18) and (1000,4) not aligned: 18 (dim 1) != 1000 (dim 0)"
     ]
    }
   ],
   "source": [
    "# TODO: You can change most things in this cell if you want to, we encurage it!\n",
    "\n",
    "n = 1000\n",
    "d = 4\n",
    "\n",
    "k = np.random.randint(0,10,(d,1))\n",
    "x = np.random.normal(0,1,(n,d))\n",
    "y = np.dot(x,k) + 0.1 + np.random.normal(0,0.01,(n,1))\n",
    "\n",
    "                        \n",
    "nn = NeuralNet(d, 1, [18, 12])\n",
    "\n",
    "loss_1 = [nn.adjust_weights(x, y) for _ in range(1000)] \n",
    "\n",
    "loss_2 = train_net(...) # TODO: Use the train_net function to compare with the \"adjust_weights\" function.\n",
    "\n",
    "plt.plot(loss_1)\n",
    "plt.title(\"Loss 1\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss_2)\n",
    "plt.title(\"Loss 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5d17c1",
   "metadata": {},
   "source": [
    "### Real test and preprocessing\n",
    "\n",
    "When using real data and neural networks, it is very important to scale the data between smaller values, usually between 0 and 1. This is because neural networks struggle with larger values as input compared to smaller values. \n",
    "\n",
    "To test this, we will use our first dataset and test with and without scaling.\n",
    "\n",
    "Similar as with assignment 2, we will use the scikit-learn library for this preprocessing: https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bddc1c7",
   "metadata": {},
   "source": [
    "### 8) Dataset 1: Wine - with and without scaling\n",
    "\n",
    "Read about the Wine dataset: https://archive.ics.uci.edu/dataset/109/wine\n",
    "\n",
    "Train two neural network, one with scaling and one without. Are we able to see any difference in training results or loss over time?\n",
    "\n",
    "**Note:** Do not train for to many epochs (more than maybe 50-100). The network might \"learn\" anyway in the end, but you should still be able to see a difference when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6154fdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing # For one-hot-encoding and scaling if needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_wine = pd.read_csv(\"wine.csv\").to_numpy()\n",
    "\n",
    "# TODO: Set up the data and split it into train and test-sets.\n",
    "\n",
    "# TODO: Train and test your neural networks.\n",
    "# NOTE: Use the same train/test split for both neural network models!\n",
    "\n",
    "# TODO: Do the above at least 3 times\n",
    "# NOTE: Use loops here!\n",
    "\n",
    "# TODO: Plot the results with matplotlib (plt)\n",
    "# NOTE: One combined lineplot with the scaling and one without the scaling, 2 plots in total.\n",
    "# NOTE: Plot both the accuracy and the loss!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39ab63",
   "metadata": {},
   "source": [
    "## Real data and hyper-parameter tuning\n",
    "\n",
    "Now we are going to use real data, preprocess it, and do hyper-parameter tuning.\n",
    "\n",
    "Choose two hyper-parameters to tune to try and achive an even better result. **Each chosen parameter needs to have at least 3 different values**. For example, if we were to choose \"epochs\" then we could test 10, 50, and 100 (optionally more). \n",
    "\n",
    "To better see what each hyper-parameter does to the models performance, we recommend doing a *grid-search* on the two chosen parameters. A grid-search tests how the two parameters interact with eachother. There might be a setting in parameter 1 that in combination with a setting in parameter 2 makes the model perform significantly better, compared to the other combinations. A grid-search test all these combinations.\n",
    "\n",
    "**IMPORTANT NOTE:** Changing the **number of epochs** can be part of the tuning, but it **does not count** towards the two hyper parameters you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6a7c7a",
   "metadata": {},
   "source": [
    "### 9) Dataset 2: Mushroom\n",
    "\n",
    "Read about the Mushroom dataset: https://archive.ics.uci.edu/dataset/73/mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a861ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing # For one-hot-encoding and scaling if needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_mushroom = pd.read_csv(\"mushroom.csv\").to_numpy()\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "# NOTE: You can choose how to handle data with missing values. Either remove or \"fix\" the missing data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f363a",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e9d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8991d79",
   "metadata": {},
   "source": [
    "### 10) Dataset 3: Adult\n",
    "\n",
    "Read about the Adult dataset: https://archive.ics.uci.edu/dataset/2/adult\n",
    "\n",
    "**IMPORTANT NOTE:** This dataset is much larger than the previous two (48843 instances). If your code runs slow on your own computer, either run this on the compute-server, or you may exclude parts of this dataset. But you must keep a minimum of 10000 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80862998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing # For one-hot-encoding and scaling if needed\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_3 = pd.read(...) # TODO: Read the data.\n",
    "\n",
    "# TODO: Preprocess the data.\n",
    "# NOTE: You can choose how to handle data with missing values. Either remove or \"fix\" the missing data.\n",
    "\n",
    "# TODO: Split the data into train and test\n",
    "\n",
    "# TODO: Train a neural network on the data\n",
    "\n",
    "# TODO: Visualize the loss for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9bd1ff",
   "metadata": {},
   "source": [
    "When hyper-parameter tuning, please write the parameters and network sizes you test here:\n",
    "\n",
    "* Parameter 1: \n",
    "* Parameter 2:\n",
    "\n",
    "* Neural network sizes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5229f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameter tuning\n",
    "\n",
    "# TODO: Visualize the loss after hyper-parameter tuning for each epoch\n",
    "\n",
    "# TODO: Visulaize the test accuracy after hyper-parameter tuning for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df892ad",
   "metadata": {},
   "source": [
    "# Questions for examination:\n",
    "\n",
    "In addition to completing the assignment with all its tasks, you should also prepare to answer the following questions:\n",
    "\n",
    "1) Why would we want to use different activation functions?\n",
    "\n",
    "2) Why would we want to use different loss functions?\n",
    "\n",
    "3) Why are neural networks sensitive to large input values?\n",
    "\n",
    "4) What is the role of the bias? \n",
    "\n",
    "5) What is the purpose of hyper-parameter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cf459",
   "metadata": {},
   "source": [
    "# Finished!\n",
    "\n",
    "Was part of the setup incorrect? Did you spot any inconsistencies in the assignment? Could something improve?\n",
    "\n",
    "If so, please write them and send via email and send it to:\n",
    "\n",
    "* marcus.gullstrand@ju.se\n",
    "\n",
    "Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
